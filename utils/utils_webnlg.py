from typing import List, Any
import copy
import os

from addict import Dict as adict
from xml.etree.ElementTree import Element, SubElement

import torch
from torch.utils.data import Dataset

from dataset.webnlg.benchmark_reader import Benchmark
from dataset.webnlg_cards import WebNLGCard
from dataset.webnlg_dataset import WebNLGDataset
from utils.utils import save_json
from utils.utils_xml import xml_prettify
from utils.utils_webnlg_parse import (get_triples)

import logging

logger = logging.getLogger(__name__)
log = logger


def save_webnlg_text(hyps: Any,
                     targets: Any,
                     out_dir: str):
    ''' Saves hypotheses and targets to file '''
    tgt_fname = os.path.join(out_dir, "target")
    hyp_fname = os.path.join(out_dir, "hypothesis")

    log.info(f"creating target     file : [{tgt_fname}]")
    log.info(f"creating hypothesis file : [{hyp_fname}]")

    # save hypotheses
    with open(hyp_fname, 'w') as fp:
        for hyp in hyps:
            hyp = hyp.strip()
            fp.write(hyp + '\n')

    # save targets
    with open(tgt_fname, 'w') as fp:
        for ref in targets:
            ref = ref.strip()
            fp.write(ref + '\n')

    # return name of files (if needed by caller)
    out = {'targets': tgt_fname,
           'hypotheses': hyp_fname}

    return out


def create_xml(data, meta_infos, ts_header, t_header):
    ''' From data, creates a benchmark object'''

    benchmark = Element('benchmark')
    entries = SubElement(benchmark, 'entries')

    assert len(meta_infos) == len(data)

    for idx, triples in enumerate(data):

        meta = meta_infos[idx]
        category = meta['category']
        eid = meta['eid']
        shape = meta['shape']
        shape_type = meta['shape_type']
        size = meta['size']

        entry = SubElement(entries, 'entry',
                           attrib={'category': category, 'eid': eid, 'shape': shape, 'shape_type': shape_type,
                                   'size': size})
        t_entry = SubElement(entry, ts_header)

        for triple in triples:
            element = SubElement(t_entry, t_header)
            element.text = triple

    return benchmark


def get_webnlg_rdf_generated_filenames(out_dir: str):
    ''' Returns the names of the files generated by Text2RDF (T2G)'''
    tgts_triples_json = os.path.join(out_dir, "target.triples.json")
    hyps_triples_json = os.path.join(out_dir, "hypothesis.triples.json")

    tgts_fname_xml = os.path.join(out_dir, "targets.xml")
    hyps_fname_xml = os.path.join(out_dir, "hyps.xml")

    out = adict({'targets_triples_json': tgts_triples_json,
                 'hypotheses_triples_json': hyps_triples_json,
                 "targets_xml": tgts_fname_xml,
                 "hypotheses_xml": hyps_fname_xml})

    return out


def get_webnlg_text_generated_filenames(out_dir: str):
    ''' Returns the names of the files generated by RDF2Text (G2T) '''

    tgts_fname = os.path.join(out_dir, "target")
    hyps_fname = os.path.join(out_dir, "hypothesis")

    out = adict({'targets': tgts_fname,
                 'hypotheses': hyps_fname})
    return out


def save_webnlg_rdf(hyps: Any,
                    targets: Any,
                    meta_infos: List,
                    out_dir: str,
                    prepare: str = 'webnlg'):
    ''' Saves hypotheses and targets graphs to files'''

    tgts = get_triples(targets)
    hyps = get_triples(hyps)

    filesD = get_webnlg_rdf_generated_filenames(out_dir)
    tgts_triples_json = filesD.targets_triples_json
    hyps_triples_json = filesD.hypotheses_triples_json
    tgts_fname_xml = filesD.targets_xml
    hyps_fname_xml = filesD.hypotheses_xml

    save_json(tgts_triples_json, tgts, msg='parsed target triples:', logger=log)
    save_json(hyps_triples_json, hyps, msg='parsed hypothesis triples:', logger=log)

    if len(tgts) != len(hyps):
        raise Exception(f"targets size {len(tgts)} is not same as hypothesis size {len(hyps)}")

    tgts_xml_tree = create_xml(tgts, meta_infos, "modifiedtripleset", "mtriple")
    hyps_xml_tree = create_xml(hyps, meta_infos, "generatedtripleset", "gtriple")  # required by WebNLG 2020 Challenge

    log.info(f"# Creating targets xml  file : [{tgts_fname_xml}]")
    log.info(f"# Creating hypothesis xml file : [{hyps_fname_xml}]")

    with open(tgts_fname_xml, 'w', encoding='utf-8') as f:
        f.write(xml_prettify(tgts_xml_tree))

    with open(hyps_fname_xml, 'w', encoding='utf-8') as f:
        f.write(xml_prettify(hyps_xml_tree))

    return filesD


def webnlg_text_args(dir_name, split, subset='all'):
    ''' Returns a dot dict w/ parameters need for webnlg text evaluation
    Args:
    dir_name: input dir (read only)
    split: testB or valB
    subset: 'all', 'seen', or 'unseen'
    '''

    is_val = "val" in split
    args = adict()
    args.language = 'en'

    if subset == 'all':

        log.info('# using *official* reference from WebNLG')
        reference_dir = WebNLGCard.references_val if is_val else WebNLGCard.references_test
        args.reference = os.path.join(reference_dir, 'reference')
        args.hypothesis = os.path.join(dir_name, 'hypothesis')

    elif (not is_val) and subset == 'seen':

        reference_dir = WebNLGCard.references_test_seen
        args.reference = os.path.join(reference_dir, 'reference')  # prefix for reference0,1,.
        args.hypothesis = os.path.join(dir_name, 'hypothesis.seen')

    elif (not is_val) and subset == 'unseen':

        reference_dir = WebNLGCard.references_test_unseen
        args.reference = os.path.join(reference_dir, 'reference')  # prefix for reference0,1,.
        args.hypothesis = os.path.join(dir_name, 'hypothesis.unseen')

    else:
        raise RuntimeError(f'invalid subset {subset} / split {split} combination')

    # args.numrefs: see https://www.aclweb.org/anthology/2020.webnlg-1.7.pdf section 4.1, pg 6 1st column, 2nd paragraph
    # "For both considered languages, the participating systems were
    # automatically evaluated in a multi-reference scenario. Each
    # English hypothesis was compared with a maximum of 5 references,
    # and each Russian one with a maximum of 7 references "
    args.num_refs = 5  # default number of references -- as defined for the WebNLG 2020 Challenge
    args.metrics = 'bleu,meteor,chrf++'
    args.ncorder = 6
    args.nworder = 2
    args.beta = 2.0

    return args


def get_references(split):
    '''
    Returns reference for given split of WebNLG dataset

    split: webnlg split to use ['val', 'test', 'train']
    returns list of text and list of triples
    '''
    valid_splits = ['val', 'testA', 'testB', 'train']

    if split not in valid_splits:
        raise RuntimeError(f'split {split} not in valid splits: {valid_splits}')

    # get .xml files
    files = WebNLGDataset.get_files(split)

    # initialize Benchmark object
    b = Benchmark()
    b.fill_benchmark(files)

    # Returns a List[List] of references from a Benchmark
    text_refs, triple_refs = gather_references(b, 'en')

    # checks
    if split == 'train' and (len(text_refs) != WebNLGCard.train_triple_set):
        raise ValueError(
            f" size of text reference for '{split}' split should be {WebNLGCard.train_triple_set} not {len(text_refs)}")
    elif split == 'test' and (len(text_refs) != WebNLGCard.test_triple_set):
        raise ValueError(
            f" size of text reference for '{split}' split should be {WebNLGCard.test_triple_set} not {len(text_refs)}")
    elif split == 'val' and (len(text_refs) != WebNLGCard.val_triple_set):
        raise ValueError(
            f" size of text reference for '{split}' split should be {WebNLGCard.val_triple_set} not {len(text_refs)}")
    else:
        pass

    if len(text_refs) != len(triple_refs):
        raise RuntimeError(
            f'text references [{len(text_refs)}] and triples references [{len(triple_refs)}] do not match counts')

    return text_refs, triple_refs


def gather_references(b, lang):
    '''
    Gather references from a benchmark
    b: instance of Benchmark class
    lang: en, ru
    '''
    text_gt = []  # List[List]
    graph_gt = []
    for eid, entry in enumerate(b.entries):

        # text
        # code below is coming from generate_references from:
        # https://gitlab.com/webnlg/corpus-reader/-/blob/master/generate_references.py#L8
        entry_refs = []
        if lang == 'en':
            for lex in entry.lexs:
                entry_refs.append(lex.lex)
        elif lang == 'ru':
            for lex in entry.lexs[1::2]:  # take every second lexicalisation, i.e. only ru
                entry_refs.append(lex.lex)
        else:
            log.info('unknown language')
        # take only unique references (needed for Russian)
        unique_refs = list(dict.fromkeys(entry_refs))
        text_gt.append(unique_refs)

        # triples
        triples = []
        for _, triple in enumerate(entry.modifiedtripleset.triples):
            triple = (triple.s, triple.p, triple.o)
            triples.append(triple)
        graph_gt.append(triples)

    return text_gt, graph_gt


def create_benchmark_from_xml(xml_fpath):
    ''' Creates a benchmark object from a xml file'''
    xml_dir = os.path.dirname(xml_fpath)
    xml_file = os.path.basename(xml_fpath)
    file_ = [(xml_dir, xml_file)]
    # initialise Benchmark object
    b = Benchmark()
    # load file to Benchmark
    b.fill_benchmark(file_)

    return b


def filter_generated_benchmark(bench_tree, categories):
    ''' Given a generated benchmark tree w/ entries w/ 'generatedtripleset' subelement,
    filter the entries (i.e. keep) that have a category within categories (List)
    Args:
      bench: generated benchmark tree) w/ entries w/ 'generatedtripleset' subelement (this is *not* a regular benchmark!)
      categories: List() of categories to keep
    '''

    root_input = bench_tree.getroot()
    log.info(f'# benchmark input [{len(root_input[0])}]')

    # make a copy of input benchmark
    b2 = copy.deepcopy(bench_tree)

    # not filtering to do, return the exact copy
    if not categories:
        return b2

    # filter in categories (keep entry with category in categories list)
    root = b2.getroot()
    entries_num = len(root[0])

    for i in reversed(range(entries_num)):
        # reverse the indicesos to avoid distrupting the indices when deleting entries
        entry = root[0][i]
        category = entry.attrib['category']
        if category not in categories:
            root[0].remove(entry)

    if len(root[0]):
        log.info(f'# benchmark filtered [{len(root[0])}]')
    else:
        raise RuntimeError(f"filtering benchmark with categories '{categories}' led to an empty benchmark")

    return b2


class WebNLGHFWrapDataset(Dataset):
    ''' WebNLG wrapper class for HuggingFace dataset '''

    def __init__(self, dataset_hf, tokenizer):
        self.tokenizer = tokenizer
        self.dataset_hf = dataset_hf

    def __getitem__(self, index):
        entry = self.dataset_hf[index]
        batch = (torch.as_tensor(entry['input_ids']),
                 torch.as_tensor(entry['attention_mask']),
                 torch.as_tensor(entry['input_ids']),  # bogus entry
                 torch.as_tensor(entry['attention_mask']),  # bogus entry
                 int(index))
        return batch

    def get_meta(self, index):
        return None

    def __len__(self):
        return len(self.dataset_hf)
